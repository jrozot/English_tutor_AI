# üìä Informe de Entrenamiento del Modelo de Correcci√≥n Gramatical

Este documento resume el proceso de entrenamiento simulado de un modelo de correcci√≥n gramatical en ingl√©s basado en una arquitectura tipo Transformer. El prop√≥sito fue desarrollar un componente central que pueda integrarse a herramientas educativas de apoyo al aprendizaje del ingl√©s en zonas rurales de Colombia.

---

## üß† Arquitectura del Modelo

- **Tipo**: Transformer Encoder
- **Capas**: 6
- **Tama√±o de capa oculta**: 512 unidades
- **Dropout**: 0.1
- **Par√°metros totales (simulados)**: ~25M
- **Framework**: PyTorch (simulado)
- **Idioma de entrenamiento**: Ingl√©s (en-US)

---

## üóÉÔ∏è Datos Utilizados

> ‚ö†Ô∏è Nota: los archivos de datos no est√°n incluidos debido a restricciones de tama√±o en GitHub.

- **Fuente**: [English Grammar Errors Corpus (Kaggle)](https://www.kaggle.com/datasets/yakobchuk/english-grammar-errors-corpus)
- **Tama√±o simulado del corpus**:
  - **Ejemplos de entrenamiento**: 50,000 frases con errores
  - **Ejemplos de validaci√≥n**: 5,000 frases
- **Preprocesamiento aplicado**:
  - Normalizaci√≥n de texto
  - Tokenizaci√≥n por sub-palabras
  - Eliminaci√≥n de ruido

---

## ‚öôÔ∏è Hiperpar√°metros

| Par√°metro         | Valor     |
|-------------------|-----------|
| Epochs            | 3         |
| Batch Size        | 32        |
| Optimizer         | Adam      |
| Learning Rate     | 0.0005    |
| Warmup Steps      | 500       |
| Max Length        | 64 tokens |

---

## üìà Resultados Simulados

| M√©trica     | Valor       |
|-------------|-------------|
| Accuracy    | 87.5%       |
| F1 Score    | 0.89        |
| Precision   | 0.91        |
| Recall      | 0.88        |
| Tiempo total de entrenamiento | ~5 minutos (simulado) |

---

## üìâ Ejemplo de Aprendizaje

```text
Input:    "She go to school every days."
Target:   "She goes to school every day."
Predicci√≥n: "She goes to school every day." ‚úÖ
